{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db685b16-a732-4a91-8849-57c0620ad1bb",
   "metadata": {},
   "source": [
    "# Side notes to comeback and refreshhhh üß†üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e4cd2e0-8cac-47e3-af45-fc4d9b2f8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d2bde-4ae5-4f29-9195-0e28939d2770",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Probability vs. Likelihood__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bac1ca-7f8b-4fd9-8808-cc7939221b2d",
   "metadata": {},
   "source": [
    "\n",
    "- __Probability__ corresponds to finding the chance of observing a particular outcome given a specific probability distribution of the data. In other words, it is the likelihood of observing the data under a given model.\n",
    "\n",
    "- __Likelihood__ corresponds to finding how likely a particular set of parameters of a statistical model is, given the observed data. In simple terms, it involves adjusting the parameters of the model to maximize the probability of the observed data.\n",
    "\n",
    "__Probability__ is used to determine the chance of occurrence of a particular event given a model, whereas __likelihood__ is used to estimate the parameters of the model that make the observed data most probable.\n",
    "\n",
    "When training a model, we optimize the likelihood, meaning we find the parameters that maximize the likelihood function. Once the model is trained, it can be used to compute the probability of future events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fb5b4-e3ad-429c-8a72-454b69e01a8c",
   "metadata": {},
   "source": [
    "## __Negative Log Likelihood vs. Cross-Entropy__\n",
    "\n",
    "__Always prefer__ ```torch.nn.functional.cross_entropy``` __implementation since it is much more efficient and much more numerically stable.__ See demonstration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c1bdc50-a2ae-452e-aea4-2be8fe7e99cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12]), torch.Size([12, 37]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 12\n",
    "num_classes = 37\n",
    "Y = torch.randint(low=0, high=num_classes, size=(B,))\n",
    "dummy_nnet = lambda x: torch.randn((B, num_classes))\n",
    "x = None \n",
    "logits = dummy_nnet(x)\n",
    "Y.shape, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb9c3b-97e0-40c3-a8ed-9b9becb5c049",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ebe7356-6992-40ef-8a17-98be7d529c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(logits, Y):\n",
    "    B, _ = logits.shape\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    assert probs.sum(dim=1).allclose(torch.ones((logits.shape[0]))), 'Probability must sum to 1'\n",
    "    # Negative log likelihood\n",
    "    log_probs = probs[torch.arange(B), Y].log()\n",
    "    negative_log_probs = -log_probs\n",
    "    loss_reduced = negative_log_probs.mean() # loss reduction\n",
    "    #loss_reduced = -probs[torch.arange(B), Y].log().mean() # One liner implementation\n",
    "    torch_nll_loss = torch.nn.functional.nll_loss(probs.log(), Y, reduction='mean')\n",
    "    assert loss_reduced.allclose(torch_nll_loss), 'Torch and own implementation must be equal'\n",
    "    return loss_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed80a62c-eb18-4751-a121-49cd147b8e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3732)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = negative_log_likelihood(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fbc98-f3bf-4f35-8359-2eb492be75ae",
   "metadata": {},
   "source": [
    "### Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7be007b5-e3d9-49f5-805a-648ef1a2e7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3732)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits, Y, reduction='mean')\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc880ee-a57f-4372-9872-2c04caf358fd",
   "metadata": {},
   "source": [
    "## __Dead Neurons__\n",
    "\n",
    "Dead neurons never get a gradient because for all examples they never activate (e.g., flat region in ReLU, or flat regions in Tanh and Sigmoid). See [Video](https://youtu.be/P6sfmUTpUmc?t=779) for more details. May be caused by chance at initialization or by using a high learning rate that knocks out the neuron out of data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2900f-4381-4018-a377-30083a245012",
   "metadata": {},
   "source": [
    "## __Batch Norm__\n",
    "\n",
    "As mentioned in the original paper, it accelerates (very) deep NN training by reducing the internal covariate shift of activations. It just achieves to have layer outputs that are gaussian distributions with 0 mean and 1 std. It should be placed after the layer (e.g., nn.Linear, nn.Conv2D,...) and before the activation (nn.ReLU, nn.TanH). However, some experiments have demonstrated good results applying after it... why this works? still a mistery.\n",
    "\n",
    "When using BN, the preceding layer does not need a bias term.\n",
    "\n",
    "- __momentum__. BN mean and std is updated using a EMA at each mini-batch through entire dataset. If the batch-size is small (e.g., 32) a momentum of 0.1 might be too agressive and harm training. Consider using a smaller momentum *0.001 in that case.\n",
    "\n",
    "See [Video](https://youtu.be/P6sfmUTpUmc?t=2440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f524350-f8ed-4004-b74f-40995ff800ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4464db-8320-4f9a-8458-efe87ea1fe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper01",
   "language": "python",
   "name": "paper01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
